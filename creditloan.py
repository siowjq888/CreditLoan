# -*- coding: utf-8 -*-
"""AI Assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cTRIASkzChwG48TemST9ychMLKolNFg8

# Welcome to Colab!


## Access Popular LLMs via Google-Colab-AI Without an API Key
Users with Colab's paid plans have free access to most popular LLMs via google-colab-ai Python library. For more details, refer to the [getting started with google colab ai](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/Getting_started_with_google_colab_ai.ipynb).

```
from google.colab import ai
response = ai.generate_text("What is the capital of France?")
print(response)
```


## Explore the Gemini API
The Gemini API gives you access to Gemini models created by Google DeepMind. Gemini models are built from the ground up to be multimodal, so you can reason seamlessly across text, images, code, and audio.

**How to get started?**
*  Go to [Google AI Studio](https://aistudio.google.com/) and log in with your Google account.
*  [Create an API key](https://aistudio.google.com/app/apikey).
* Use a quickstart for [Python](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb), or call the REST API using [curl](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Prompting_REST.ipynb).

**Discover Gemini's advanced capabilities**
*  Play with Gemini [multimodal outputs](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Image-out.ipynb), mixing text and images in an iterative way.
*  Discover the [multimodal Live API](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.ipynb ) (demo [here](https://aistudio.google.com/live)).
*  Learn how to [analyze images and detect items in your pictures](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb") using Gemini (bonus, there's a [3D version](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Spatial_understanding_3d.ipynb) as well!).
*  Unlock the power of [Gemini thinking model](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb), capable of solving complex task with its inner thoughts.
      
**Explore complex use cases**
*  Use [Gemini grounding capabilities](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Search_grounding_for_research_report.ipynb) to create a report on a company based on what the model can find on internet.
*  Extract [invoices and form data from PDF](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Pdf_structured_outputs_on_invoices_and_forms.ipynb) in a structured way.
*  Create [illustrations based on a whole book](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Book_illustration.ipynb) using Gemini large context window and Imagen.

To learn more, check out the [Gemini cookbook](https://github.com/google-gemini/cookbook) or visit the [Gemini API documentation](https://ai.google.dev/docs/).
"""

import nltk
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
import math
import re

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB

"""Step1: Prepare the data"""

dr = pd.read_csv("/content/test.csv")
dr

def to_num_like(s: pd.Series) -> pd.Series:
    return pd.to_numeric(
        s.astype(str).str.replace(r"[^0-9.\-]", "", regex=True),
        errors="coerce"
    )

for c in ["Monthly_Balance","Changed_Credit_Limit",
          "Amount_invested_monthly","Num_of_Delayed_Payment"]:
    if c in dr.columns:
        dr[c] = to_num_like(dr[c])

def parse_age_to_months(s):
    if not isinstance(s, str): return np.nan
    y = re.search(r"(\d+)\s*Years?", s)
    m = re.search(r"(\d+)\s*Months?", s)
    years = int(y.group(1)) if y else 0
    months = int(m.group(1)) if m else 0
    return years*12 + months

if "Credit_History_Age" in dr.columns:
    dr["Credit_History_Age_Months"] = dr["Credit_History_Age"].apply(parse_age_to_months)

def month_to_int(m):
    if not isinstance(m, str): return np.nan
    names = ["January","February","March","April","May","June","July",
             "August","September","October","November","December"]
    m2 = m.strip().title()
    if m2 in names: return names.index(m2) + 1
    for i, name in enumerate(names, start=1):
        if m2.startswith(name[:3]): return i
    return np.nan

if "Month" in dr.columns and dr["Month"].dtype == "object":
    dr["Month_num"] = dr["Month"].apply(month_to_int)

if "Type_of_Loan" in dr.columns:
    dr["Num_Loan_Types"] = dr["Type_of_Loan"].apply(
        lambda s: 0 if (pd.isna(s) or str(s).strip()=="")
        else len([p for p in str(s).split(",") if p.strip()])
    )

print(dr.columns)
print(dr.describe(include="all"))
print(dr.info())
print(dr.isnull().sum())
print(f"The dataset has {len(dr)} rows of data.")

num_cols = dr.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = dr.select_dtypes(exclude=[np.number]).columns.tolist()
dr_sample = dr.sample(min(len(dr), 8000), random_state=42)

sns.countplot(x="Credit_Mix", data=dr)
plt.title("Credit_Mix Distribution")
plt.tight_layout()
plt.show()
print(dr["Credit_Mix"].value_counts(dropna=False))

def plot_boxplots_grid(data, cols, cols_per_row=4, batch_size=12, title_prefix="Numeric"):
    cols = [c for c in cols if data[c].dropna().size > 0]
    for i in range(0, len(cols), batch_size):
        sub = cols[i:i+batch_size]
        rows = math.ceil(len(sub) / cols_per_row)
        fig, axes = plt.subplots(rows, cols_per_row, figsize=(4.5*cols_per_row, 3.8*rows))
        axes = np.array(axes).reshape(-1)
        for ax, col in zip(axes, sub):
            sns.boxplot(y=data[col], ax=ax)
            ax.set_title(col, fontsize=10)
            ax.set_xlabel("")
        for k in range(len(sub), len(axes)):
            fig.delaxes(axes[k])
        fig.suptitle(f"{title_prefix} Boxplots ({i+1}–{i+len(sub)} of {len(cols)})", y=1.02, fontsize=12)
        plt.subplots_adjust(hspace=0.6, wspace=0.35)
        plt.show()

def plot_categorical_bars(data, cols, top_n=15, cols_per_row=3, batch_size=9, title_prefix="Categorical"):
    cols = [c for c in cols if data[c].notna().any()]
    for i in range(0, len(cols), batch_size):
        sub = cols[i:i+batch_size]
        rows = math.ceil(len(sub) / cols_per_row)
        fig, axes = plt.subplots(rows, cols_per_row, figsize=(5.5*cols_per_row, 4.2*rows))
        axes = np.array(axes).reshape(-1)
        for ax, col in zip(axes, sub):
            vc = data[col].astype(str).value_counts().nlargest(top_n)[::-1]
            ax.barh(vc.index, vc.values)
            ax.set_title(f"{col} (Top {min(top_n, len(vc))})", fontsize=10)
            ax.tick_params(axis='y', labelsize=9)
        for k in range(len(sub), len(axes)):
            fig.delaxes(axes[k])
        fig.suptitle(f"{title_prefix} Counts ({i+1}–{i+len(sub)} of {len(cols)})", y=1.02, fontsize=12)
        plt.subplots_adjust(hspace=0.6, wspace=0.35)
        plt.show()

plot_boxplots_grid(dr_sample, num_cols, cols_per_row=4, batch_size=12, title_prefix="Numeric")
plot_categorical_bars(dr_sample, cat_cols, top_n=15, cols_per_row=3, batch_size=9, title_prefix="Categorical")

use_cols = [c for c in [
    "Annual_Income","Monthly_Inhand_Salary","Outstanding_Debt",
    "Total_EMI_per_month","Credit_Utilization_Ratio","Credit_History_Age_Months"
] if c in dr.columns]
if use_cols:
    sns.pairplot(dr[use_cols].dropna().sample(min(3000, len(dr)), random_state=42))
    plt.show()

fig = px.pie(dr, names="Credit_Mix", title="Credit_Mix Share")
fig.show()

x = pd.to_numeric(dr['Annual_Income'], errors='coerce')
y = pd.to_numeric(dr['Monthly_Inhand_Salary'], errors='coerce')
mask = x.notna() & y.notna()
x, y = x[mask], y[mask]
if len(x) > 50000:
    idx = np.random.RandomState(42).choice(len(x), size=50000, replace=False)
    x, y = x.iloc[idx], y.iloc[idx]
fig, ax = plt.subplots(figsize=(9, 6))
ax.scatter(x, y, s=8, alpha=0.5, edgecolors='none', rasterized=True, zorder=2)
ax.set_xlabel('Annual_Income'); ax.set_ylabel('Monthly_Inhand_Salary')
ax.set_title('Monthly_Inhand_Salary vs Annual_Income'); ax.grid(True)
plt.tight_layout(); plt.show()

if {"Num_Credit_Inquiries","Credit_History_Age_Months","Credit_Mix"}.issubset(dr.columns):
    fig = px.scatter(dr, x="Num_Credit_Inquiries", y="Credit_History_Age_Months",
                     color="Credit_Mix", title="Credit History (months) vs Num Credit Inquiries")
    fig.show()

"""Step 2&3 Feature Extraction and Split the data"""

drop_ids = [c for c in ["ID","Customer_ID","Name","SSN"] if c in dr.columns]
X = dr.drop(columns=drop_ids + ["Credit_Mix"])
y = dr["Credit_Mix"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

num_cols = X_train.select_dtypes(include=np.number).columns.tolist()
cat_all  = X_train.select_dtypes(exclude=np.number).columns.tolist()
high_card = [c for c in cat_all if X_train[c].nunique(dropna=True) > 60]
cat_cols  = [c for c in cat_all if c not in high_card]
print("Dropped high-card categorical columns:", high_card)

numeric_tf = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler()),
])

try:
    ohe = OneHotEncoder(handle_unknown="infrequent_if_exist", min_frequency=50, sparse_output=True)
except TypeError:
    ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=True, max_categories=50)


categorical_tf = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", ohe),
])

preprocess_full = ColumnTransformer([
    ("num", numeric_tf, num_cols),
    ("cat", categorical_tf, cat_cols),
], remainder="drop")

preprocess_num = ColumnTransformer([
    ("num", numeric_tf, num_cols),
], remainder="drop")

from sklearn.linear_model import LogisticRegression

logreg = Pipeline([("preprocess", preprocess_full),
                   ("model", LogisticRegression(max_iter=1000, class_weight="balanced"))])

knn = Pipeline([("preprocess", preprocess_num),
                ("model", KNeighborsClassifier(n_neighbors=5, weights="distance"))])

logreg.fit(X_train, y_train)
y_pred_lr = logreg.predict(X_test)

print("Num/Cat:", len(num_cols), "/", len(cat_cols))
print("Splits:", X_train.shape, X_test.shape)

"""Step 4: Idenfity the model"""

x_train_numeric = X_train.select_dtypes(include=np.number)
x_test_numeric  = X_test.select_dtypes(include=np.number)

imputer = SimpleImputer(strategy='median')
x_train_imputed = imputer.fit_transform(x_train_numeric)
x_test_imputed  = imputer.transform(x_test_numeric)

scaler = StandardScaler()
x_train_processed = scaler.fit_transform(x_train_imputed)
x_test_processed  = scaler.transform(x_test_imputed)

label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded  = label_encoder.transform(y_test)

def eval_and_plot(name, y_true_enc, y_pred_enc, label_encoder):
    acc = accuracy_score(y_true_enc, y_pred_enc)
    pr, rc, f1, _ = precision_recall_fscore_support(y_true_enc, y_pred_enc, average='macro', zero_division=0)
    print(f"\n== {name} ==\nAccuracy: {acc:.4f} | Macro P/R/F1: {pr:.4f}/{rc:.4f}/{f1:.4f}")
    print(classification_report(y_true_enc, y_pred_enc,
                                target_names=label_encoder.classes_, zero_division=0))
    ConfusionMatrixDisplay.from_predictions(
        label_encoder.inverse_transform(y_true_enc),
        label_encoder.inverse_transform(y_pred_enc),
        colorbar=True
    )
    plt.title(f"Confusion Matrix — {name}")
    plt.tight_layout(); plt.show()
    return acc, pr, rc, f1

"""Gradient Boosting"""

try:
    Xtr_tree, Xte_tree = x_train_imputed, x_test_imputed
except NameError:
    Xtr_tree, Xte_tree = x_train_processed, x_test_processed

extra_rows = []

gb = GradientBoostingClassifier(random_state=42)
gb.fit(Xtr_tree, y_train_encoded)
pred_gb = gb.predict(Xte_tree)
acc_gb, pr_gb, rc_gb, f1_gb = eval_and_plot("Gradient Boosting", y_test_encoded, pred_gb, label_encoder)
extra_rows.append(["Gradient Boosting", acc_gb, pr_gb, rc_gb, f1_gb])

"""Random Forest"""

rf = RandomForestClassifier(n_estimators=300, max_depth=None, n_jobs=-1,
                            class_weight="balanced_subsample", random_state=42)
rf.fit(Xtr_tree, y_train_encoded)
pred_rf = rf.predict(Xte_tree)
acc_rf, pr_rf, rc_rf, f1_rf = eval_and_plot("Random Forest", y_test_encoded, pred_rf, label_encoder)
extra_rows.append(["Random Forest", acc_rf, pr_rf, rc_rf, f1_rf])

"""MLP"""

from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(hidden_layer_sizes=(64,32), max_iter=500,
                    early_stopping=True, random_state=42)
mlp.fit(x_train_processed, y_train_encoded)
pred_mlp = mlp.predict(x_test_processed)
acc_mlp, pr_mlp, rc_mlp, f1_mlp = eval_and_plot("MLP (64,32)", y_test_encoded, pred_mlp, label_encoder)

"""Summary"""

summary = pd.DataFrame({
    "Model": ["Gradient Boosting","Random Forest","MLP (64,32)"],
    "Accuracy": [acc_gb, acc_rf, acc_mlp],
    "Macro_Precision": [pr_gb, pr_rf, pr_mlp],
    "Macro_Recall": [rc_gb, rc_rf, rc_mlp],
    "Macro_F1": [f1_gb, f1_rf, f1_mlp],
})
display(summary.sort_values("Macro_F1", ascending=False))

plt.figure(figsize=(8,5))
plt.bar(summary["Model"], summary["Accuracy"], label="Accuracy")
plt.bar(summary["Model"], summary["Macro_F1"], alpha=0.6, label="Macro F1")
plt.ylabel("Score"); plt.title("Model Comparison")
plt.legend(); plt.tight_layout(); plt.show()

models = {
    "Gradient Boosting": gb,
    "Random Forest": rf,
    "MLP (64,32)": mlp
}

best_model_name = summary.sort_values("Macro_F1", ascending=False).iloc[0]["Model"]
best_model = models[best_model_name]

print(f"Best model is: {best_model_name}")

from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
import joblib

# Recreate model (depending on best_model_name)
if best_model_name == "Gradient Boosting":
    best_model_fresh = GradientBoostingClassifier(random_state=42)
elif best_model_name == "Random Forest":
    best_model_fresh = RandomForestClassifier(
        n_estimators=300, max_depth=None, n_jobs=-1,
        class_weight="balanced_subsample", random_state=42
    )
elif best_model_name == "MLP (64,32)":
    best_model_fresh = MLPClassifier(hidden_layer_sizes=(64, 32),
                                     max_iter=500,
                                     early_stopping=True,
                                     random_state=42)

# Build pipeline with preprocessing + best model
final_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),  # Handle missing values
    ("scaler", StandardScaler()),  # Standardize features (scale)
    ("model", best_model_fresh)  # The selected model
])

# Fit pipeline on numeric training data (raw numeric, not pre-scaled)
final_pipeline.fit(x_train_numeric, y_train_encoded)

# Save pipeline instead of raw model
joblib.dump(final_pipeline, "model.pkl")
joblib.dump(label_encoder, "label_encoder.pkl")
print("✅ Pipeline model and Label Encoder saved successfully!")

import gradio as gr
import joblib
import numpy as np
import time

# Load model and label encoder
start_time = time.time()
pipeline = joblib.load("model.pkl")
label_encoder = joblib.load("label_encoder.pkl")
end_time = time.time()
print(f"Model loaded in {end_time - start_time:.2f} seconds.")

# Prediction function
def predict_fn(monthly_inhand_salary, num_bank_accounts, num_credit_card, interest_rate,
               delay_from_due_date, num_of_delayed_payment, changed_credit_limit,
               num_credit_inquiries, credit_utilization_ratio, total_emi_per_month,
               amount_invested_monthly, monthly_balance, credit_history_age_months,
               month_num, num_loan_types):
    try:
        print("Received inputs:", monthly_inhand_salary, num_bank_accounts, num_credit_card,
              interest_rate, delay_from_due_date, num_of_delayed_payment, changed_credit_limit,
              num_credit_inquiries, credit_utilization_ratio, total_emi_per_month,
              amount_invested_monthly, monthly_balance, credit_history_age_months,
              month_num, num_loan_types)

        # Prepare the data (make sure it matches the training data)
        data = np.array([[monthly_inhand_salary, num_bank_accounts, num_credit_card,
                          interest_rate, delay_from_due_date, num_of_delayed_payment,
                          changed_credit_limit, num_credit_inquiries, credit_utilization_ratio,
                          total_emi_per_month, amount_invested_monthly, monthly_balance,
                          credit_history_age_months, month_num, num_loan_types]])


        # Make prediction using the loaded pipeline
        prediction = pipeline.predict(data)
        print("Prediction result:", prediction)

        # Convert numerical prediction back to label (Good/Standard/Bad)
        if isinstance(prediction[0], str):
            label = prediction[0]
        else:
            label = label_encoder.inverse_transform(prediction)[0]

        return f"Predicted Credit Mix: {label}"

    except Exception as e:
        print("❌ Error during prediction:", str(e))
        return f"Error: {str(e)}"

# Gradio UI with 15 numerical inputs
iface = gr.Interface(
    fn=predict_fn,
    inputs=[
        gr.Number(label="Monthly Inhand Salary"),
        gr.Number(label="Number of Bank Accounts"),
        gr.Number(label="Number of Credit Cards"),
        gr.Number(label="Interest Rate"),
        gr.Number(label="Delay from Due Date"),
        gr.Number(label="Number of Delayed Payment"),
        gr.Number(label="Changed Credit Limit"),
        gr.Number(label="Number of Credit Inquiries"),
        gr.Number(label="Credit Utilization Ratio"),
        gr.Number(label="Total EMI per Month"),
        gr.Number(label="Amount Invested Monthly"),
        gr.Number(label="Monthly Balance"),
        gr.Number(label="Credit History Age (Months)"),
        gr.Number(label="Month Number"),
        gr.Number(label="Number of Loan Types"),
    ],
    outputs="text",
    title="Credit Loan Prediction App",
    description="Enter financial details to predict the customer's Credit Mix (Good, Standard, Bad)."
)

# Launch the Gradio interface
iface.launch()

